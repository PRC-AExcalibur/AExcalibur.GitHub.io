---
title:  【机器学习】降维:PCA
categories:
- MachineLearning
tags:
- ComputerScience 
- MachineLearning 
---
主成分分析(PCA)教程。


---
# 主成分分析（PCA）

### 
主成分分析 (PCA) 是一种统计方法，用于通过正交变换将一组可能相关的变量转换成一组线性不相关的变量，称为主成分。

#### 目的
- 降维：减少数据集的维度，同时尽可能保留原始数据的变异性。
- 去相关：将原始数据转换为一组不相关的变量。


#### 数学工具
##### 向量表示与基变换
- **内积**：两个向量的点积可以解释为一个向量在另一个向量上的投影长度乘以后者的模。
- **基**：向量空间中的一组向量，用来表示空间中的任何向量。基的选择会影响向量的坐标表示。
- **基变换**：将数据从原始基变换到新基的过程。

矩阵乘法可以用来表示基变换，左边矩阵的行向量表示新基，右边矩阵的列向量表示原始数据。

##### 方差（Variance）
方差是衡量数据集中各个数据点与均值之间差异的平方的平均值。它表示数据的离散程度。
对于一组数据 $ X = \{x_1, x_2, ..., x_n\} $，其方差 $ \sigma^2 $ 定义为：
$$ \sigma^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \mu)^2 $$
其中，$ \mu $ 是数据集的均值。

##### 协方差（Covariance）
协方差是衡量两个变量联合变化趋势的度量。
如果两个变量的增减趋势一致，则协方差为正；如果一个变量增加时另一个变量减少，则协方差为负。
两个随机变量 $ X $ 和 $ Y $ 的协方差 $ \text{Cov}(X, Y) $ 定义为：
$$ \text{Cov}(X, Y) = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \mu_X)(y_i - \mu_Y) $$
其中，$ \mu_X $ 和 $ \mu_Y $ 分别是 $ X $ 和 $ Y $ 的均值。
协方差可以表示两个变量的相关性。

##### 协方差矩阵（Covariance Matrix）
协方差矩阵是一个方阵，用于描述多个随机变量之间的协方差关系。
设有 $ p $ 个随机变量 $ X_1, X_2, ..., X_p $，它们的协方差矩阵 $ \Sigma $ 是一个 $ p \times p $ 的矩阵，其元素定义如下：
$$ \Sigma_{ij} = \text{Cov}(X_i, X_j) $$
对角线上的元素 $ \Sigma_{ii} $ 是变量 $ X_i $ 的方差，非对角线上的元素 $ \Sigma_{ij} $（$ i \neq j $）是 $ X_i $ 和 $ X_j $ 的协方差。
显然，协方差矩阵是对称矩阵。

#### 最大可分性

如果进行基变换后，新基的数量少于向量本身的维数，这个变换就对数据进行了降维。

选择基的优化条件：希望在新基下，数据的投影尽可能分散，即方差最大化。

可以通过选择 K 个单位正交基，将一组 N 维向量降为 K 维，使原始数据变换到新基后：
- 各变量两两之间协方差为 0；
- 变量方差尽可能大。（即在正交约束下，取最大的 K 个方差）

如果我们使用P作为基变换矩阵，将X变换为Y，即 $Y = PX$, 那么我们可以推导出Y的协方差矩阵D与X的协方差矩阵C之间的关系:
$$ D = \frac{1}{n-1}YY^T = \frac{1}{n-1}(PX)(PX)^T = \frac{1}{n-1}PX X^T P^T = PCP^T$$

可以发现 P 是能让原始协方差矩阵对角化的矩阵。

由于协方差矩阵C是对称矩阵，其不同特征值对应特征向量正交，因此只要求协方差矩阵的特征值和特征向量即可。

协方差矩阵的特征值表示在对应主成分上的方差，进行递减的排序；
前k个特征值对应的特征向量将作为新基。

设P为协方差矩阵的新基单位化后按行排列出的矩阵，其中每一行对应一个基。
用 P 的前 K 行组成的矩阵乘以原始数据矩阵 X，就得到了需要的降维后的数据矩阵 Y。

### PCA 的步骤
1. 数据标准化：确保每个特征的均值为0。（即减去这一行的均值）
2. 计算协方差矩阵。
3. 求解协方差矩阵的特征值和特征向量。
4. 选择前K个最大的特征值对应的特征向量作为新基，组成矩阵 P。
5. 将原始数据投影到新基上，Y=PX 得到主成分。


### 优点
- 缓解维度灾难：通过降低数据的维度，PCA增加了样本在新空间中的采样密度，从而缓解了随着维度增加而样本密度急剧下降的维度灾难问题。
- 降噪：数据中的噪声可能与较小的特征值对应的特征向量相关。
  通过舍弃这些特征向量，PCA可以在一定程度上减少噪声的影响。
- 特征独立性：PCA不仅降低数据维度，还通过正交变换生成一组线性不相关的特征，即新特征之间相互独立，这有助于简化模型并减少多重共线性问题。

### 缺点
- 过拟合风险：尽管PCA去除了噪声和不重要的特征，但它可能过度拟合训练数据。
一些在训练集上表现不明显但对泛化能力重要的信息可能被错误地视为无用信息而被舍弃。

### PCA 的应用
- 数据压缩：去除不重要的信息，减少数据存储需求。
- 特征工程：在机器学习中作为预处理步骤。
