---
title:  【机器学习】集成学习
categories:
- MachineLearning
tags:
- ComputerScience 
- MachineLearning 
---
集成学习教程。


---
# 集成学习（Ensemble Learning）

### 集成学习介绍
**定义**：集成学习是一种机器学习范式，它通过结合多个基学习器（弱学习器）的预测来提高整体模型的性能。

**目的**：改善单个学习器的识别率和泛化能力，减少过拟合和提高模型的鲁棒性。

#### 集成学习框架
- **Bagging（自举汇聚法）**：从原始训练集中进行有放回抽样创建多个子集，每个子集训练一个基学习器，最终综合结果。常用的综合方法是投票法。
- **Boosting（提升法）**：按顺序逐步训练基学习器，后续学习器在前续学习器的基础上进行调整，以关注被前续学习器错分的样本。常用的综合方法为加权法。
- **Stacking（堆叠法）**：首先用全部数据训练多个基学习器，然后使用这些学习器的预测结果作为新特征，训练一个学习器来做出最终预测。


#### 偏差（Bias）与方差（Variance）
集成学习通过结合多个模型的预测来提高整体性能，这个过程涉及到偏差（bias）和方差（variance）的权衡。

##### 模型总体期望和方差
假设有$ m $个基模型，每个基模型的预测为$ \hat{y}_i $，权重为$ w_i $，期望为$ \mu_i $，方差为$ \sigma_i^2 $，两两模型间的相关系数为$ \rho $。

- **模型总体期望**：
  $$ \text{E}[\hat{Y}] = \sum_{i=1}^{m} w_i \mu_i $$

- **模型总体方差**（考虑协方差）：
  $$ \text{Var}(\hat{Y}) = \sum_{i=1}^{m} w_i^2 \sigma_i^2 + \sum_{i=1}^{m}\sum_{j=1, j \neq i}^{m} w_i w_j \text{Cov}(\hat{y}_i, \hat{y}_j) $$
  
  由于$ \text{Cov}(\hat{y}_i, \hat{y}_j) = \rho \sigma_i \sigma_j $，可以简化为：
  $$ \text{Var}(\hat{Y}) =  \left(\frac{\sigma_i^2}{m}\right) + \frac{m-1}{m} \rho \sigma^2 $$
  其中，$ \sigma^2 $是基模型的平均方差。

##### Bagging的偏差与方差
Bagging中基模型权重相等，且基模型间相对独立（𝜌接近0）。
- 模型总体期望：等于单个基模型的期望。
- 模型总体方差：小于单个基模型的方差，随着𝑚的增加而减小。

随着基模型数量增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高。

##### Boosting的偏差与方差
Boosting中基模型间强相关（𝜌接近1）。
- 模型总体期望：期望随着基模型数的增加而增加，同时整体模型的准确度提高。
- 模型总体方差：等于单个基模型的方差，因为基模型间的强相关性抵消了方差的减少。

如果基模型不是弱模型，其方差相对较大，这将导致整体模型的方差很大，即无法达到防止过拟合的效果。因此，Boosting 框架中的基模型必须为弱模型。

#### 常见集成学习算法
- **随机森林（Random Forest）**：一种Bagging方法，使用决策树作为基学习器，通过构建多个决策树并进行投票或平均来提高预测的准确性。
- **AdaBoost（自适应Boosting）**：一种Boosting方法，通过逐步增加对错误分类样本的权重来提高模型性能。
- **梯度提升决策树（GBDT）**：一种Boosting方法，通过构建决策树来最小化损失函数的梯度。
- ...

#### 优点
- **提高准确性**：通过结合多个模型的预测，可以减少模型的偏差和方差。
- **减少过拟合**：集成学习可以平滑个别模型的极端预测，降低过拟合的风险。
- **提高鲁棒性**：集成多个模型可以减少对单个模型错误或异常值的敏感性。

#### 缺点
- **计算成本**：训练多个基学习器可能需要大量的计算资源。
- **模型复杂性**：管理和优化多个模型可能比单一模型更复杂。

### 随机森林

**定义**：随机森林是一种基于决策树的集成学习方法，它通过Bagging方法减少模型的方差，并引入随机特征选择进一步增加模型的多样性。

随机森林的流程如下：
1. **随机选择样本**：使用Bootstrap抽样方法进行有放回抽样，为每棵决策树创建不同的训练集。
2. **随机选择特征**：在决策树的每个分裂节点上，随机选择一部分特征，然后从中选择最佳分裂特征。
3. **构建决策树**：每棵树基于其对应的样本和特征进行训练，直至不能进一步分裂。
4. **投票（分类）/平均（回归）**：所有决策树独立完成预测，最终结果通过投票或平均的方式确定。

随机森林的随机性导致每棵树的偏差略有增加，但通过模型的“平均”特性，方差显著减小，从而提高了整体模型的性能。

#### 优点
- **高准确性**：在多种数据集上表现优异，通常优于单一决策树和其他算法。
- **易于并行化**：由于每棵树独立训练，随机森林易于在多核或分布式系统上并行化，适合处理大数据集。
- **高维数据处理**：无需进行特征选择，能够处理具有大量特征的高维数据集。
- **防止过拟合**：通过随机采样和多样性，即使不剪枝，单棵树也不太可能过拟合。

#### 缺点
- 相比于单棵决策树，随机森林可能会引入更多的模型复杂性。
- 由于模型包含大量树，模型的解释性较差。


### AdaBoost
**定义**：AdaBoost是一种自适应的迭代算法，通过逐步增加对错误分类样本的权重来增强弱分类器的性能。

AdaBoost的流程如下：
1. **初始化**：给定初始样本权重分布，通常所有样本权重相同。
2. **迭代训练**：在每一轮中，训练一个弱分类器，根据分类结果更新样本权重，错误分类的样本权重增加，正确分类的样本权重减少。
   - 损失函数：AdaBoost使用指数损失函数来度量分类误差。
   - 前向分步学习：每一轮的弱学习器基于前一轮的误差来更新样本权重。
   - 正则化：为了防止过拟合，AdaBoost引入步长参数来控制每一步更新的强度。
   - 弱分类器权重 $ \alpha_k $ 由公式 $ \alpha_k = \frac{1}{2} \ln\left(\frac{1 - \epsilon_k}{\epsilon_k}\right) $ 确定，其中 $ \epsilon_k $ 是第 $ k $ 个弱分类器的错误率。
   - 样本权重更新：使用 $ w_{i, k+1} = w_{i, k} \exp(\alpha_k \cdot y_i \cdot \hat{y}_{i, k}) $，其中 $ y_i $ 是真实标签，$ \hat{y}_{i, k} $ 是第 $ k $ 个弱分类器的预测。
3. **组合弱分类器**：通过加权组合所有弱分类器的预测结果，形成最终的强分类器。
   - 加法模型：最终分类器是多个弱分类器的加权和。



#### 优点：
- 高分类精度，特别适用于分类问题。
- 灵活性高，可以使用不同的弱学习器。
- 通常具有较好的泛化能力，不易过拟合。
#### 缺点：
- 对异常值敏感，异常点可能获得较高权重，影响整体性能。
